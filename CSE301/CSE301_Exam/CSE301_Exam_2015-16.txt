----------------------------Part1----------------------------
Q1. After training a Self-Organizing Map, output neurons that win for similar input are usually far apart from each other in the map.
False

Q2. The Hopfield Network has only a single layer of neurons.
True

Q3. Clustering can be useful when we want to analyse a large number of pattern vecotrs and identify groups of patterns with similar features
True

Q4. Patterns within a cluster should be similar in some way.
True

Q5. Clusters that are similar in some way should be far apart.
False

Q6. Artifical neurons are identical in opeartion to biological ones.
False

Q6. A perceptron adds up all the weighted inputs it receives, and if it exceeds a certain value, it outputs a 1, otherwise it just outputs a 0.
True

Q7. A neural network with multiple hidden layers and sigmoid nodes can form non-linear decision boundaries.
True

Q8. The training time depends on the size of the network.
True

Q9. The perceptron training precedure is guaranteed to converge if the two classes are linearly separable.
True

Q10. Backpropagation learninig is guaranteed to converge.
False

Q11. A neural network with one hidden layer that contains only polynomial number of (hidden) nodes can represent any Boolean function.
False

Q12. A multiple-layer neural network with linear activation functions is equivalent to one single-layer perceptron taht uses the same error function on the output layer and has the same number of inputs.
True

Q13. Convolution is a linear operation i.e. (af1+bf2) * g = af1 * g + bf2 * g.
True

Q14. A mlp has to have the same number of input nodes as output nodes.
False

Q15. Multilayer perceptrons have full connectivity between the layers.
True

Q16. The number of connections in the network scales linearly with the number of nodes.
False

Q17. For arbitrary neural networks, with weights optimized using a stochastic gradient method, setting weights to 0 is an acceptable initialization.
False

Q18. An advantage with gradient descent based methods, such as back propagation, is that they cannot get stuck in local minima.
False

Q19. Convolutional Neural Network(CNN) is just another classification model with higher accuracies.
False

----------------------------Part2----------------------------
Q1. An autoassociative network is: 
    (a) a neural network that contains no loops.
    (b) a neural network that contains feedback.
    (c) a neural network that has only one loop.
Answer: B. An autoassociative network is equivalent to a neural network taht contains feedback. The number of feedback paths(loops) does not have to be one

Q2. Which of the following equations is the best description of the Perceptron Learning Rule?
    (a) △wk = nykx
    (b) △wk = n(x-wk)
    (c) △wk = n(dk-yk)x
Answer: C 

Q3. If the associated pattern pairs(x,y0 are differentand if the model recalls a y given an x or vice versa, then it is termed as
    (a) Auto correlator
    (b) Auto-associative memory
    (c) Heteroassociative memory
    (d) Double associative memory
Answer: C

Q4. An artificial neural network used for learning efficient codings by treaching the network to reproduce the provided input data is called
    (a) Autoencoder, autoassociator or Diabolo network
    (b) Radial Basis Function(RBF) network
    (c) Multi layered perceptron
Answer: A

Q5. How many hidden layers are there in an autoassociative Hopfield network?
    (a) None
    (b) One
    (c) Two
Answer: A

Q6. A Hopfield network has 20 units. How many adjustable parameters does this network contain?
    (a) 95
    (b) 190
    (c) 200
    (d) 380
Answer: B. 顶点和边的关系: N个定点有N(N-1)/2条边. 所以, 20*(20-1)/2=190

Q7. Which one of the following statements about Oja's rule is NOT ture?
    (a) Oja's rule converges asymptotically by introducing a 'forgetting term' that prevents the weights from growing without bounds.
    (b) Oja's rule itself cannot find more than one principal components in the data.
    (c) Oja's rule results in unstable learning. i,e., the magnitude of the weight vector increases without bounds. 
    (d) A linear neuron trained with Oja's rule produces as weight vector that is the eigenvector of the input auto-correlation matrix.
Answer: C

Q8. Which of the following statements in NOT true for hard competitive learning(HCL)?
    (a) There is no target output in HCL.
    (b) There are no hidden units in a HCL network.
    (c) The input vectors are often normalized to have unit length -- that is, x = 1.
    (d) The weights of the winning unit k are adapted  by 4wk = _i(x-wj). where _i < _ and j6=k.(我不知道这里发生了什么...)
    (e) The weights of the neighbours j of the winning unit are adapted by 4wj=_i(x-wj). where _i<_ and j6=k. (我也不知道这里经历了什么...)
Answer: E

Q9. Which of the following statements is NOT true for a self-organizing feature map(SOM)
    (a) The size of the neighbourhood is decreased during training.
    (b) The SOM training algorithm is based on soft competitive learning.
    (c) The network can grow during training by adding new cluster units when required.
    (d) The cluster units are arranged in a regular geometric pattern such as a square or ring.
    (e) The learning rate is a function of the distance of the adapted units from the winning unit.
Answer: C

Q10. Which one of the following matrices is the weight matrix for a Hopfield network to store the pattern [1 -1 1 -1]
    (a)
    (b)
    (c)
    (d)
Answer: 答案多少不重要, 方法才最重要: 将pattern与自己的转置矩阵外积就是答案, 如果有多个pattern, 就外积完相加在一起.

Q11. A self-organizing feature map(SOM) has 8 input units, and 100 output units arranged in a two dimensional grid. How many weights does this network have?
    (a) 80
    (b) 100
    (c) 800
    (d) 1000
    (e) 1500
Answer: C 8*100=800

Q12. A self-organizing feature map(SOM) has 8 input units, and 25 output units arranged in a two dimensional grid. How many weights does this network have?
    (a) 20
    (b) 25
    (c) 200
    (d) 250
    (e) 800
Answer: C 8*25=200

Q13. A self-organizing feature map has four cluster units arranged in a one-dimensional ring, as shown in the following diagram(形容下: 一个环, 上面逆时针等距分布Unit1-4). The weight vectors of the four units are given as follows:
W1 = [-1,-1.5,0.5]
W2 = [2,-2,5.2]
W3 = [1.5,6,4.3]
W4 = [-4,7,0.6]
An input vector x = [-1.4,2.3,0.2] is presented to the network. Which unit is nearest to x in terms of Euclidean distance?
    (a) Unit 1
    (b) Unit 2
    (c) Unit 3
    (d) Unit 4
Answer: A. 算x和w的距离就好了, 欧几里德距离公式一套就好了. 其实肉眼也能看出来第一个近一点.

Q14. Adapt the weight vector of the winning unit in last question according to the SOM learning algorithm with a learning rate of 0.5, using the same unput vector as before. What is the new weight vector?
    (a) Wwinner = [-2.7,4.65,0.4]
    (b) Wwinner = [-1.2,0.4,0.35]
    (c) Wwinner = [0.05,4.15,2.25]
    (d) Wwinner = [0.3,0.15,2.7]
Answer: B. 具体算法: 之前的winner weight 是W1, 然后我们用公式, NewWeight = OldWeight + η*(input - OldWeight)


Q15. Which of the following statements is NOT true for a self-organizing feature map (SOM)?
    (a) The size of the neighbourhood is based on soft competitive learning.
    (b) The SOM training alogrithm is based on soft competitive learning.
    (c) The cluster units are arranged in a regular geometric pattern such as a square or ring.
    (d) The learning rate is a function of the distance of the adapted units from the winning unit.
    (e) The network can grow during training by adding new cluster.
Answer: E

Q16. What is the topological mapping in a self-organizing feature map(SOM)?
    (a) A map whichorganizes the robots and tells them where to go.
    (b) A mapping where similar inputs produce similar outputs, which preserves the probability distribution of the training data.
    (c) An approximation of a continuous function, whihc maps the input vectors onto their posterior probabilities.
    (d) A mapping where similar inputs produce different outputs, which preserves the possibility distribution of the training data.
Answer: B

----------------------------Part3----------------------------
Q1. What is Hebb's rule?
Answer: This is the simplest way of training an artificial neuron: the synapse between two neurons is reinforced if there is co-activity, or equivalently if for a given neuron its input is 1 and simultaneously its output is 1.
wj = wj + xjy
For neurons with binary 0/1 states, the weight is updated only if positive activity takes place. But the rule can also be applied to "neurons" with +1/-1 states and for linear discriminant with continuous value inputs.
If there are the same numbers of examples in either class, a linear classifier f(x) = w*x+b trained with Hebb's rule classifies examples according to the nearest class centroid. It may classify the training examples with a few errors.

Q2. What is a spurious stable state of a Hopfield net?
Answer: a stable state which is not designed into the system.

Q3. Write down the energy function of a discrete Hopfield net.
Answer: E = -0.5∑i=1->N ∑j=1->N wjixjxi

Q4. What is "overfitting"?
Answer: Learning the training examples very well but making poor predictions on the new test examples.

Q5. What is the difference between supervised learning and unsupervised learning?
Answer: In supervised learning, training patterns giving inputs and corresponding correct outputs are available, while in unsupervised learning, the system must find interesting and/or significant patterns in the data without any feedback as to what is "right".

Q6. What is meant by the term attractor in a Hopfield network? What is special about a spurious attractor.
Answer: These appear in Hopfield networks. These are stable states that do not correspond to any learned pattern. They appear as a side effect of learning. They also depend on correlations between patterns.

Q7. What are the weights w0, w1, and w2 for the perceptron whose decision surface is illustrated below. You should assume that the decision surface crosses the X1 axis at -5 and corsses the X2 axis at 8.

                   X2
                    ^ + /            
                    |  /            
          +         |+/            
                    |/            
          +     +   /          - 
                   /|            
               +  / |          - 
        +        /  |    -       
                /   |            
---------------/----|------------------------>X1
              /     |            
        +    /      |   -        
            /       |            
           /    -   |            
          /   -     | -          
                    |            

w0 + 8w2 = 0
w0 - 5w1 = 0
wo < 0
Then we will get w0 = -40, w1 = -8, w2 = 5
or anything proportional.

Q8. A 2-layer feed-forward neural network with 5 input units, 3 hidden units and 2 output units contains how many weights? (Include biases) Show your work.
Answer: ((5+1)*3)+((3+1)*2) = 18 + 8 = 26
